{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaba6e16",
   "metadata": {},
   "source": [
    "## Pydantic Ai Adapter in Action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c244c354",
   "metadata": {},
   "source": [
    "#### Setup Path and Reload Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb58b2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Environment configured\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/home/timmy/RepoAI_AI/.env', override=True)\n",
    "\n",
    "# Enable autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"✓ Environment configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09066316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Modules cleared from cache\n"
     ]
    }
   ],
   "source": [
    "# Force reload modules\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "# Remove cached modules\n",
    "if 'repoai.llm.pydantic_ai_adapter' in sys.modules:\n",
    "    del sys.modules['repoai.llm.pydantic_ai_adapter']\n",
    "if 'repoai.llm.router' in sys.modules:\n",
    "    del sys.modules['repoai.llm.router']\n",
    "\n",
    "print(\"✓ Modules cleared from cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cad7eb",
   "metadata": {},
   "source": [
    "#### Setup Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "256ce561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Logging configured\n"
     ]
    }
   ],
   "source": [
    "from repoai.utils.logger import setup_logging\n",
    "import logging\n",
    "\n",
    "# Setup with DEBUG level to see all the fallback attempts and model selections\n",
    "setup_logging(level=logging.INFO, use_colors=True)\n",
    "\n",
    "print(\"✓ Logging configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c38543",
   "metadata": {},
   "source": [
    "#### Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c335a379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Modules imported\n"
     ]
    }
   ],
   "source": [
    "from repoai.llm.pydantic_ai_adapter import PydanticAIAdapter\n",
    "from repoai.llm.model_roles import ModelRole\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "print(\"✓ Modules imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e60aeaf",
   "metadata": {},
   "source": [
    "#### Test Schema for Structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71665dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Schemas defined\n"
     ]
    }
   ],
   "source": [
    "class CodeSuggestion(BaseModel):\n",
    "    language: str\n",
    "    code: str\n",
    "    explanation: str\n",
    "\n",
    "class TaskList(BaseModel):\n",
    "    tasks: List[str]\n",
    "    priority: str\n",
    "\n",
    "class APIExplanation(BaseModel):\n",
    "    concept: str\n",
    "    definition: str\n",
    "    key_features: List[str]\n",
    "    example_use_case: str\n",
    "\n",
    "print(\"✓ Schemas defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742a78f0",
   "metadata": {},
   "source": [
    "#### Initialize Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "533e384d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-24 20:15:32 | INFO     | repoai.llm.router | ModelRouter Initialized.\n",
      "✓ PydanticAIAdapter initialized\n",
      "\n",
      "Router configuration:\n",
      "{'INTAKE': {'primary': 'deepseek/deepseek-chat-v3.1', 'fallbacks': ['alibaba/qwen-max', 'claude-sonnet-4-5-20250929'], 'total_models': 3}, 'PLANNER': {'primary': 'deepseek/deepseek-reasoner-v3.1', 'fallbacks': ['alibaba/qwen3-next-80b-a3b-thinking', 'claude-opus-4-20250514'], 'total_models': 3}, 'PR_NARRATOR': {'primary': 'deepseek/deepseek-chat-v3.1', 'fallbacks': ['claude-haiku-4-5-20251001', 'alibaba/qwen3-235b-a22b-thinking-2507'], 'total_models': 3}, 'CODER': {'primary': 'alibaba/qwen3-coder-480b-a35b-instruct', 'fallbacks': ['Qwen/Qwen2.5-Coder-32B-Instruct', 'deepseek/deepseek-chat-v3.1', 'claude-opus-4-1-20250805'], 'total_models': 4}, 'EMBEDDING': {'primary': 'bge-small', 'fallbacks': [], 'total_models': 1}}\n",
      "✓ PydanticAIAdapter initialized\n",
      "\n",
      "Router configuration:\n",
      "{'INTAKE': {'primary': 'deepseek/deepseek-chat-v3.1', 'fallbacks': ['alibaba/qwen-max', 'claude-sonnet-4-5-20250929'], 'total_models': 3}, 'PLANNER': {'primary': 'deepseek/deepseek-reasoner-v3.1', 'fallbacks': ['alibaba/qwen3-next-80b-a3b-thinking', 'claude-opus-4-20250514'], 'total_models': 3}, 'PR_NARRATOR': {'primary': 'deepseek/deepseek-chat-v3.1', 'fallbacks': ['claude-haiku-4-5-20251001', 'alibaba/qwen3-235b-a22b-thinking-2507'], 'total_models': 3}, 'CODER': {'primary': 'alibaba/qwen3-coder-480b-a35b-instruct', 'fallbacks': ['Qwen/Qwen2.5-Coder-32B-Instruct', 'deepseek/deepseek-chat-v3.1', 'claude-opus-4-1-20250805'], 'total_models': 4}, 'EMBEDDING': {'primary': 'bge-small', 'fallbacks': [], 'total_models': 1}}\n"
     ]
    }
   ],
   "source": [
    "adapter = PydanticAIAdapter()\n",
    "print(\"✓ PydanticAIAdapter initialized\")\n",
    "print(\"\\nRouter configuration:\")\n",
    "print(adapter.router.get_config_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2990f5be",
   "metadata": {},
   "source": [
    "#### Raw Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04b4bac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-24 20:00:38 | INFO     | repoai.llm.pydantic_ai_adapter | Starting raw completion: role=INTAKE, fallback=True\n",
      "2025-10-24 20:00:38 | INFO     | repoai.llm.pydantic_ai_adapter | Attempting raw completion 1/3: model=deepseek/deepseek-chat-v3.1\n",
      "2025-10-24 20:00:38 | INFO     | repoai.llm.pydantic_ai_adapter | Attempting raw completion 1/3: model=deepseek/deepseek-chat-v3.1\n",
      "2025-10-24 20:00:43 | INFO     | repoai.llm.pydantic_ai_adapter | Raw completion succeeded on attempt 1: 4785.54 ms, output length=266\n",
      "\n",
      "Raw Output: \n",
      "Python is a high-level, general-purpose programming language known for its clear and readable syntax. It is widely used for web development, data analysis, artificial intelligence, and scientific computing due to its extensive collection of libraries and frameworks.\n",
      "✓ Length: 266 characters\n",
      "2025-10-24 20:00:43 | INFO     | repoai.llm.pydantic_ai_adapter | Raw completion succeeded on attempt 1: 4785.54 ms, output length=266\n",
      "\n",
      "Raw Output: \n",
      "Python is a high-level, general-purpose programming language known for its clear and readable syntax. It is widely used for web development, data analysis, artificial intelligence, and scientific computing due to its extensive collection of libraries and frameworks.\n",
      "✓ Length: 266 characters\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain what Python is in 2 sentences.\"}\n",
    "]\n",
    "\n",
    "output = await adapter.run_raw_async(\n",
    "    ModelRole.INTAKE, \n",
    "    messages, \n",
    "    max_output_tokens=100\n",
    ")\n",
    "\n",
    "print(f\"\\nRaw Output: \\n{output}\\n✓ Length: {len(output)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e021f30",
   "metadata": {},
   "source": [
    "#### Structured Output (JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c097b047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-24 16:45:35 | INFO     | repoai.llm.pydantic_ai_adapter | Starting JSON completion: role= CODER, Schema= CodeSuggestion, use_fallback= True\n",
      "2025-10-24 16:45:35 | INFO     | repoai.llm.pydantic_ai_adapter | Attempting JSON completion 1/4: Model: OpenAIChatModel(), role: CODER\n",
      "2025-10-24 16:45:45 | INFO     | repoai.llm.pydantic_ai_adapter | JSON completion succeeded on attempt 1: 9548.47 ms\n",
      "\n",
      "Result Type: <class '__main__.CodeSuggestion'>\n",
      "✓ Is CodeSuggestion? True\n",
      "\n",
      "Language: Python\n",
      "\n",
      "Code:\n",
      "def fibonacci(n):\n",
      "    if n <= 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return fibonacci(n - 1) + fibonacci(n - 2)\n",
      "\n",
      "Explanation:\n",
      "This recursive function calculates the nth Fibonacci number by breaking down the problem into smaller subproblems. The base cases handle n=0 (returns 0) and n=1 (returns 1). For larger values, it recursively calls itself to compute the sum of the two preceding Fibonacci numbers. Note that this implementation has exponential time complexity due to repeated calculations.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a coding expert.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a Python function to calculate the nth Fibonacci number using recursion.\"}\n",
    "]\n",
    "\n",
    "result = await adapter.run_json_async(\n",
    "    role=ModelRole.CODER,\n",
    "    schema=CodeSuggestion,\n",
    "    messages=messages,\n",
    "    max_output_tokens=500\n",
    ")\n",
    "\n",
    "print(f\"\\nResult Type: {type(result)}\")\n",
    "print(f\"✓ Is CodeSuggestion? {isinstance(result, CodeSuggestion)}\")\n",
    "print(f\"\\nLanguage: {result.language}\")\n",
    "print(f\"\\nCode:\\n{result.code}\")\n",
    "print(f\"\\nExplanation:\\n{result.explanation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de6f5b0",
   "metadata": {},
   "source": [
    "#### Raw Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1425ef04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Streaming output:\n",
      "2025-10-24 16:46:22 | INFO     | repoai.llm.pydantic_ai_adapter | Starting raw streaming completion: role=INTAKE\n",
      "2025-10-24 16:46:22 | INFO     | repoai.llm.pydantic_ai_adapter | Attempting streaming 1/3: model=OpenAIChatModel()\n",
      "2025-10-24 16:46:22 | INFO     | repoai.llm.pydantic_ai_adapter | Attempting streaming 1/3: model=OpenAIChatModel()\n",
      "SilSiliconicon mind dreams,\n",
      "On a stream mind dreams,\n",
      "On a stream of ones and zeros of ones and zeros,\n",
      "New,\n",
      "New thoughts take their thoughts take their first breath.2025-10-24 16:46:25 | INFO     | repoai.llm.pydantic_ai_adapter | Streaming succeeded on attempt 1\n",
      "\n",
      "----------------------------------------\n",
      "✓ Received 7 chunks\n",
      " first breath.2025-10-24 16:46:25 | INFO     | repoai.llm.pydantic_ai_adapter | Streaming succeeded on attempt 1\n",
      "\n",
      "----------------------------------------\n",
      "✓ Received 7 chunks\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a creative writer.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a haiku about artificial intelligence.\"}\n",
    "]\n",
    "\n",
    "print(\"\\nStreaming output:\")\n",
    "\n",
    "chunk_count = 0\n",
    "async for chunk in adapter.stream_raw_async(\n",
    "    ModelRole.INTAKE, \n",
    "    messages, \n",
    "    max_output_tokens=100\n",
    "):\n",
    "    print(chunk, end='', flush=True)\n",
    "    chunk_count += 1\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(f\"✓ Received {chunk_count} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e482c2",
   "metadata": {},
   "source": [
    "#### Different Model Roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bf16628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 1/3: INTAKE\n",
      "----------------------------------------\n",
      "2025-10-24 16:48:26 | INFO     | repoai.llm.pydantic_ai_adapter | Starting raw completion: role=INTAKE, fallback=True\n",
      "2025-10-24 16:48:26 | INFO     | repoai.llm.pydantic_ai_adapter | Attempting raw completion 1/3: model=OpenAIChatModel()\n",
      "2025-10-24 16:48:26 | INFO     | repoai.llm.pydantic_ai_adapter | Attempting raw completion 1/3: model=OpenAIChatModel()\n",
      "2025-10-24 16:49:12 | INFO     | repoai.llm.pydantic_ai_adapter | Raw completion succeeded on attempt 1: 46062.03 ms, output length=5683\n",
      "Prompt: What is machine learning?\n",
      "Response: Of course! Here is a comprehensive yet easy-to-understand explanation of what machine learning is.\n",
      "\n",
      "...\n",
      "\n",
      "\n",
      "Test 2/3: PLANNER\n",
      "----------------------------------------\n",
      "2025-10-24 16:49:12 | INFO     | repoai.llm.pydantic_ai_adapter | Starting raw completion: role=PLANNER, fallback=True\n",
      "2025-10-24 16:49:12 | INFO     | repoai.llm.pydantic_ai_adapter | Attempting raw completion 1/3: model=OpenAIChatModel()\n",
      "2025-10-24 16:49:12 | INFO     | repoai.llm.pydantic_ai_adapter | Raw completion succeeded on attempt 1: 46062.03 ms, output length=5683\n",
      "Prompt: What is machine learning?\n",
      "Response: Of course! Here is a comprehensive yet easy-to-understand explanation of what machine learning is.\n",
      "\n",
      "...\n",
      "\n",
      "\n",
      "Test 2/3: PLANNER\n",
      "----------------------------------------\n",
      "2025-10-24 16:49:12 | INFO     | repoai.llm.pydantic_ai_adapter | Starting raw completion: role=PLANNER, fallback=True\n",
      "2025-10-24 16:49:12 | INFO     | repoai.llm.pydantic_ai_adapter | Attempting raw completion 1/3: model=OpenAIChatModel()\n",
      "2025-10-24 16:50:03 | INFO     | repoai.llm.pydantic_ai_adapter | Raw completion succeeded on attempt 1: 51009.60 ms, output length=2605\n",
      "Prompt: Create a plan for building a todo app\n",
      "Response: # Todo App Development Plan\n",
      "\n",
      "Here's a comprehensive plan for building a todo application, from conce...\n",
      "\n",
      "\n",
      "Test 3/3: CODER\n",
      "----------------------------------------\n",
      "2025-10-24 16:50:03 | INFO     | repoai.llm.pydantic_ai_adapter | Starting raw completion: role=CODER, fallback=True\n",
      "2025-10-24 16:50:03 | INFO     | repoai.llm.pydantic_ai_adapter | Attempting raw completion 1/4: model=OpenAIChatModel()\n",
      "2025-10-24 16:50:03 | INFO     | repoai.llm.pydantic_ai_adapter | Raw completion succeeded on attempt 1: 51009.60 ms, output length=2605\n",
      "Prompt: Create a plan for building a todo app\n",
      "Response: # Todo App Development Plan\n",
      "\n",
      "Here's a comprehensive plan for building a todo application, from conce...\n",
      "\n",
      "\n",
      "Test 3/3: CODER\n",
      "----------------------------------------\n",
      "2025-10-24 16:50:03 | INFO     | repoai.llm.pydantic_ai_adapter | Starting raw completion: role=CODER, fallback=True\n",
      "2025-10-24 16:50:03 | INFO     | repoai.llm.pydantic_ai_adapter | Attempting raw completion 1/4: model=OpenAIChatModel()\n",
      "2025-10-24 16:50:08 | INFO     | repoai.llm.pydantic_ai_adapter | Raw completion succeeded on attempt 1: 4651.28 ms, output length=716\n",
      "Prompt: Write a hello world function in Python\n",
      "Response: Here's a simple \"Hello, World!\" function in Python:\n",
      "\n",
      "```python\n",
      "def hello_world():\n",
      "    print(\"Hello, ...\n",
      "\n",
      "2025-10-24 16:50:08 | INFO     | repoai.llm.pydantic_ai_adapter | Raw completion succeeded on attempt 1: 4651.28 ms, output length=716\n",
      "Prompt: Write a hello world function in Python\n",
      "Response: Here's a simple \"Hello, World!\" function in Python:\n",
      "\n",
      "```python\n",
      "def hello_world():\n",
      "    print(\"Hello, ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_cases = [\n",
    "    (ModelRole.INTAKE, \"What is machine learning?\"),\n",
    "    (ModelRole.PLANNER, \"Create a plan for building a todo app\"),\n",
    "    (ModelRole.CODER, \"Write a hello world function in Python\"),\n",
    "]\n",
    "\n",
    "for i, (role, prompt) in enumerate(test_cases, 1):\n",
    "    print(f\"\\nTest {i}/{len(test_cases)}: {role.name}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    output = await adapter.run_raw_async(\n",
    "        role, \n",
    "        [{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_output_tokens=80\n",
    "    )\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Response: {output[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc2e1d8",
   "metadata": {},
   "source": [
    "#### Complex Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04941dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-24 16:53:59 | INFO     | repoai.llm.pydantic_ai_adapter | Starting JSON completion: role= PLANNER, Schema= APIExplanation, use_fallback= True\n",
      "2025-10-24 16:53:59 | INFO     | repoai.llm.pydantic_ai_adapter | Attempting JSON completion 1/3: Model: OpenAIChatModel(), role: PLANNER\n",
      "2025-10-24 16:53:59 | INFO     | repoai.llm.pydantic_ai_adapter | Attempting JSON completion 1/3: Model: OpenAIChatModel(), role: PLANNER\n",
      "2025-10-24 16:54:32 | INFO     | repoai.llm.pydantic_ai_adapter | JSON completion succeeded on attempt 1: 33076.75 ms\n",
      "\n",
      "Concept: REST APIs\n",
      "\n",
      "Definition:\n",
      "REST (Representational State Transfer) is an architectural style for designing networked applications that uses standard HTTP methods to interact with resources identified by URIs, enabling stateless and scalable communication between clients and servers.\n",
      "\n",
      "Key Features:\n",
      "  1. Stateless: Each request from client to server must contain all information needed, with no session state stored on the server.\n",
      "  2. Client-Server: Separation of concerns allows independent evolution of client and server components.\n",
      "  3. Cacheable: Responses define caching policies to improve performance and reduce server load.\n",
      "  4. Uniform Interface: Simplifies architecture with consistent use of HTTP methods (GET, POST, PUT, DELETE) and resource identifiers.\n",
      "  5. Layered System: Supports hierarchical layers for scalability, security, and load balancing.\n",
      "\n",
      "Example Use Case:\n",
      "A mobile app fetches real-time weather data by sending an HTTP GET request to a REST API endpoint, which returns a JSON response with current conditions and forecasts.\n",
      "2025-10-24 16:54:32 | INFO     | repoai.llm.pydantic_ai_adapter | JSON completion succeeded on attempt 1: 33076.75 ms\n",
      "\n",
      "Concept: REST APIs\n",
      "\n",
      "Definition:\n",
      "REST (Representational State Transfer) is an architectural style for designing networked applications that uses standard HTTP methods to interact with resources identified by URIs, enabling stateless and scalable communication between clients and servers.\n",
      "\n",
      "Key Features:\n",
      "  1. Stateless: Each request from client to server must contain all information needed, with no session state stored on the server.\n",
      "  2. Client-Server: Separation of concerns allows independent evolution of client and server components.\n",
      "  3. Cacheable: Responses define caching policies to improve performance and reduce server load.\n",
      "  4. Uniform Interface: Simplifies architecture with consistent use of HTTP methods (GET, POST, PUT, DELETE) and resource identifiers.\n",
      "  5. Layered System: Supports hierarchical layers for scalability, security, and load balancing.\n",
      "\n",
      "Example Use Case:\n",
      "A mobile app fetches real-time weather data by sending an HTTP GET request to a REST API endpoint, which returns a JSON response with current conditions and forecasts.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a technical educator.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain REST APIs\"}\n",
    "]\n",
    "\n",
    "result = await adapter.run_json_async(\n",
    "    role=ModelRole.PLANNER,\n",
    "    schema=APIExplanation,\n",
    "    messages=messages,\n",
    "    max_output_tokens=400\n",
    ")\n",
    "\n",
    "print(f\"\\nConcept: {result.concept}\")\n",
    "print(f\"\\nDefinition:\\n{result.definition}\")\n",
    "print(f\"\\nKey Features:\")\n",
    "for i, feature in enumerate(result.key_features, 1):\n",
    "    print(f\"  {i}. {feature}\")\n",
    "print(f\"\\nExample Use Case:\\n{result.example_use_case}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769698c0",
   "metadata": {},
   "source": [
    "#### FallBack Behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07e0a0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "With fallback enabled (default):\n",
      "2025-10-24 16:56:00 | INFO     | repoai.llm.pydantic_ai_adapter | Starting raw completion: role=INTAKE, fallback=True\n",
      "2025-10-24 16:56:00 | INFO     | repoai.llm.pydantic_ai_adapter | Attempting raw completion 1/3: model=OpenAIChatModel()\n",
      "2025-10-24 16:56:00 | INFO     | repoai.llm.pydantic_ai_adapter | Attempting raw completion 1/3: model=OpenAIChatModel()\n",
      "2025-10-24 16:56:03 | INFO     | repoai.llm.pydantic_ai_adapter | Raw completion succeeded on attempt 1: 2815.28 ms, output length=13\n",
      "✓ Response: 1, 2, 3, 4, 5\n",
      "\n",
      "============================================================\n",
      "Without fallback (primary model only):\n",
      "2025-10-24 16:56:03 | INFO     | repoai.llm.pydantic_ai_adapter | Starting raw completion: role=INTAKE, fallback=False\n",
      "2025-10-24 16:56:03 | INFO     | repoai.llm.pydantic_ai_adapter | Using primary model: OpenAIChatModel()\n",
      "2025-10-24 16:56:03 | INFO     | repoai.llm.pydantic_ai_adapter | Raw completion succeeded on attempt 1: 2815.28 ms, output length=13\n",
      "✓ Response: 1, 2, 3, 4, 5\n",
      "\n",
      "============================================================\n",
      "Without fallback (primary model only):\n",
      "2025-10-24 16:56:03 | INFO     | repoai.llm.pydantic_ai_adapter | Starting raw completion: role=INTAKE, fallback=False\n",
      "2025-10-24 16:56:03 | INFO     | repoai.llm.pydantic_ai_adapter | Using primary model: OpenAIChatModel()\n",
      "2025-10-24 16:56:03 | INFO     | repoai.llm.pydantic_ai_adapter | Raw completion succeeded: -294.00 ms. output length=13 \n",
      "✓ Response: 1, 2, 3, 4, 5\n",
      "2025-10-24 16:56:03 | INFO     | repoai.llm.pydantic_ai_adapter | Raw completion succeeded: -294.00 ms. output length=13 \n",
      "✓ Response: 1, 2, 3, 4, 5\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Count from 1 to 5\"}\n",
    "]\n",
    "\n",
    "print(\"\\nWith fallback enabled (default):\")\n",
    "output = await adapter.run_raw_async(\n",
    "    ModelRole.INTAKE,\n",
    "    messages,\n",
    "    max_output_tokens=50,\n",
    "    use_fallback=True\n",
    ")\n",
    "print(f\"✓ Response: {output}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Without fallback (primary model only):\")\n",
    "output = await adapter.run_raw_async(\n",
    "    ModelRole.INTAKE,\n",
    "    messages,\n",
    "    max_output_tokens=50,\n",
    "    use_fallback=False\n",
    ")\n",
    "print(f\"✓ Response: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12259e17",
   "metadata": {},
   "source": [
    "#### Performance Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41f45fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running performance test...\n",
      "\n",
      "2025-10-24 16:56:48 | INFO     | repoai.llm.pydantic_ai_adapter | Starting raw completion: role=INTAKE, fallback=True\n",
      "2025-10-24 16:56:48 | INFO     | repoai.llm.pydantic_ai_adapter | Attempting raw completion 1/3: model=OpenAIChatModel()\n",
      "2025-10-24 16:56:48 | INFO     | repoai.llm.pydantic_ai_adapter | Attempting raw completion 1/3: model=OpenAIChatModel()\n",
      "2025-10-24 16:57:33 | INFO     | repoai.llm.pydantic_ai_adapter | Raw completion succeeded on attempt 1: 44443.97 ms, output length=5408\n",
      "Call 1 (INTAKE):\n",
      "  Duration: 44447ms\n",
      "  Output: Of course! Here is a comprehensive overview of what Python i...\n",
      "\n",
      "2025-10-24 16:57:33 | INFO     | repoai.llm.pydantic_ai_adapter | Starting raw completion: role=CODER, fallback=True\n",
      "2025-10-24 16:57:33 | INFO     | repoai.llm.pydantic_ai_adapter | Attempting raw completion 1/4: model=OpenAIChatModel()\n",
      "2025-10-24 16:57:33 | INFO     | repoai.llm.pydantic_ai_adapter | Raw completion succeeded on attempt 1: 44443.97 ms, output length=5408\n",
      "Call 1 (INTAKE):\n",
      "  Duration: 44447ms\n",
      "  Output: Of course! Here is a comprehensive overview of what Python i...\n",
      "\n",
      "2025-10-24 16:57:33 | INFO     | repoai.llm.pydantic_ai_adapter | Starting raw completion: role=CODER, fallback=True\n",
      "2025-10-24 16:57:33 | INFO     | repoai.llm.pydantic_ai_adapter | Attempting raw completion 1/4: model=OpenAIChatModel()\n",
      "2025-10-24 16:57:39 | INFO     | repoai.llm.pydantic_ai_adapter | Raw completion succeeded on attempt 1: 5994.91 ms, output length=1138\n",
      "Call 2 (CODER):\n",
      "  Duration: 5998ms\n",
      "  Output: Here are a few different versions of a hello function:\n",
      "\n",
      "## S...\n",
      "\n",
      "2025-10-24 16:57:39 | INFO     | repoai.llm.pydantic_ai_adapter | Starting raw completion: role=PLANNER, fallback=True\n",
      "2025-10-24 16:57:39 | INFO     | repoai.llm.pydantic_ai_adapter | Attempting raw completion 1/3: model=OpenAIChatModel()\n",
      "2025-10-24 16:57:39 | INFO     | repoai.llm.pydantic_ai_adapter | Raw completion succeeded on attempt 1: 5994.91 ms, output length=1138\n",
      "Call 2 (CODER):\n",
      "  Duration: 5998ms\n",
      "  Output: Here are a few different versions of a hello function:\n",
      "\n",
      "## S...\n",
      "\n",
      "2025-10-24 16:57:39 | INFO     | repoai.llm.pydantic_ai_adapter | Starting raw completion: role=PLANNER, fallback=True\n",
      "2025-10-24 16:57:39 | INFO     | repoai.llm.pydantic_ai_adapter | Attempting raw completion 1/3: model=OpenAIChatModel()\n",
      "2025-10-24 16:59:00 | INFO     | repoai.llm.pydantic_ai_adapter | Raw completion succeeded on attempt 1: 81331.35 ms, output length=4622\n",
      "Call 3 (PLANNER):\n",
      "  Duration: 81333ms\n",
      "  Output: Here's a plan for a simple **To-Do List Web App** using HTML...\n",
      "\n",
      "✓ Completed 3 calls in 131.78s\n",
      "  Average: 43.93s per call\n",
      "2025-10-24 16:59:00 | INFO     | repoai.llm.pydantic_ai_adapter | Raw completion succeeded on attempt 1: 81331.35 ms, output length=4622\n",
      "Call 3 (PLANNER):\n",
      "  Duration: 81333ms\n",
      "  Output: Here's a plan for a simple **To-Do List Web App** using HTML...\n",
      "\n",
      "✓ Completed 3 calls in 131.78s\n",
      "  Average: 43.93s per call\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "test_cases = [\n",
    "    (ModelRole.INTAKE, {\"role\": \"user\", \"content\": \"What is Python?\"}),\n",
    "    (ModelRole.CODER, {\"role\": \"user\", \"content\": \"Write a hello function\"}),\n",
    "    (ModelRole.PLANNER, {\"role\": \"user\", \"content\": \"Plan a simple web app\"}),\n",
    "]\n",
    "\n",
    "print(\"\\nRunning performance test...\\n\")\n",
    "start = time.time()\n",
    "\n",
    "for i, (role, message) in enumerate(test_cases, 1):\n",
    "    call_start = time.time()\n",
    "    output = await adapter.run_raw_async(role, [message], max_output_tokens=60)\n",
    "    call_duration = (time.time() - call_start) * 1000\n",
    "    \n",
    "    print(f\"Call {i} ({role.name}):\")\n",
    "    print(f\"  Duration: {call_duration:.0f}ms\")\n",
    "    print(f\"  Output: {output[:60]}...\")\n",
    "    print()\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(f\"✓ Completed {len(test_cases)} calls in {elapsed:.2f}s\")\n",
    "print(f\"  Average: {elapsed/len(test_cases):.2f}s per call\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad690b5d",
   "metadata": {},
   "source": [
    "#### All Model Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "844a1b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model for CODER role: alibaba/qwen3-coder-480b-a35b-instruct\n",
      "\n",
      "All model IDs for INTAKE role:\n",
      "  1. deepseek/deepseek-chat-v3.1\n",
      "  2. alibaba/qwen-max\n",
      "  3. claude-sonnet-4-5-20250929\n",
      "\n",
      "Settings for PLANNER role:\n",
      "  Temperature: 0.3\n",
      "  Max Tokens: 4096\n",
      "\n",
      "Full spec for CODER:\n",
      "  Model ID: alibaba/qwen3-coder-480b-a35b-instruct\n",
      "  Provider: Qwen\n",
      "  Temperature: 0.2\n",
      "  Max Output Tokens: 2048\n",
      "  JSON Mode: False\n"
     ]
    }
   ],
   "source": [
    "# Get model for custom agent creation\n",
    "spec = adapter.get_spec(ModelRole.CODER)\n",
    "print(f\"\\nModel for CODER role: {spec.model_id}\")\n",
    "\n",
    "# Get all model IDs with fallback\n",
    "model_ids = adapter.get_model_ids_with_fallback(ModelRole.INTAKE)\n",
    "print(f\"\\nAll model IDs for INTAKE role:\")\n",
    "for i, model_id in enumerate(model_ids, 1):\n",
    "    print(f\"  {i}. {model_id}\")\n",
    "\n",
    "# Get model settings\n",
    "settings = adapter.get_model_settings(ModelRole.PLANNER)\n",
    "print(f\"\\nSettings for PLANNER role:\")\n",
    "print(f\"  Temperature: {settings['temperature']}\")\n",
    "print(f\"  Max Tokens: {settings['max_tokens']}\")\n",
    "\n",
    "# Get full spec\n",
    "spec = adapter.get_spec(ModelRole.CODER)\n",
    "print(f\"\\nFull spec for CODER:\")\n",
    "print(f\"  Model ID: {spec.model_id}\")\n",
    "print(f\"  Provider: {spec.provider}\")\n",
    "print(f\"  Temperature: {spec.temperature}\")\n",
    "print(f\"  Max Output Tokens: {spec.max_output_tokens}\")\n",
    "print(f\"  JSON Mode: {spec.json_mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061c251c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69f0f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RepoAI_AI (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
